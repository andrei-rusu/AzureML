{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24ce5c80-cd49-4ac4-80bb-e019deaa59f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/dbfs/deepml_lab': No such file or directory\n--2024-04-04 14:08:33--  https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/penguins.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9533 (9.3K) [text/plain]\nSaving to: ‘/dbfs/deepml_lab/penguins.csv’\n\n     0K .........                                             100% 1.96M=0.005s\n\n2024-04-04 14:08:33 (1.96 MB/s) - ‘/dbfs/deepml_lab/penguins.csv’ saved [9533/9533]\n\n"
     ]
    }
   ],
   "source": [
    " %sh\n",
    " rm -r /dbfs/deepml_lab\n",
    " mkdir /dbfs/deepml_lab\n",
    " wget -O /dbfs/deepml_lab/penguins.csv https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/penguins.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb0d4d2-a2bd-430a-8b38-d350adf64412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 957 rows, Test Set: 411 rows \n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "   \n",
    "# Load the data, removing any incomplete rows\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/deepml_lab/penguins.csv\").dropna()\n",
    "   \n",
    "# Encode the Island with a simple integer index\n",
    "# Scale FlipperLength and BodyMass so they're on a similar scale to the bill measurements\n",
    "islands = df.select(collect_set(\"Island\").alias('Islands')).first()['Islands']\n",
    "island_indexes = [(islands[i], i) for i in range(0, len(islands))]\n",
    "df_indexes = spark.createDataFrame(island_indexes).toDF('Island', 'IslandIdx')\n",
    "data = df.join(df_indexes, ['Island'], 'left').select(col(\"IslandIdx\"),\n",
    "                   col(\"CulmenLength\").astype(\"float\"),\n",
    "                   col(\"CulmenDepth\").astype(\"float\"),\n",
    "                   (col(\"FlipperLength\").astype(\"float\")/10).alias(\"FlipperScaled\"),\n",
    "                    (col(\"BodyMass\").astype(\"float\")/100).alias(\"MassScaled\"),\n",
    "                   col(\"Species\").astype(\"int\")\n",
    "                    )\n",
    "   \n",
    "# Oversample the dataframe to triple its size\n",
    "# (Deep learning techniques like LOTS of data)\n",
    "for i in range(1,3):\n",
    "    data = data.union(data)\n",
    "   \n",
    "# Split the data into training and testing datasets   \n",
    "features = ['IslandIdx','CulmenLength','CulmenDepth','FlipperScaled','MassScaled']\n",
    "label = 'Species'\n",
    "      \n",
    "# Split data 70%-30% into training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.toPandas()[features].values,\n",
    "                                                    data.toPandas()[label].values,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=0)\n",
    "   \n",
    "print ('Training Set: %d rows, Test Set: %d rows \\n' % (len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3bea880-4307-498d-92aa-308512648ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported - ready to use PyTorch 2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as td\n",
    "import torch.nn.functional as F\n",
    "   \n",
    "# Set random seed for reproducability\n",
    "torch.manual_seed(0)\n",
    "   \n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43675c74-ab9e-4f75-8974-31c75fac31d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to load data\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset and loader for the training data and labels\n",
    "train_x = torch.Tensor(x_train).float()\n",
    "train_y = torch.Tensor(y_train).long()\n",
    "train_ds = td.TensorDataset(train_x,train_y)\n",
    "train_loader = td.DataLoader(train_ds, batch_size=20,\n",
    "    shuffle=False, num_workers=1)\n",
    "\n",
    "# Create a dataset and loader for the test data and labels\n",
    "test_x = torch.Tensor(x_test).float()\n",
    "test_y = torch.Tensor(y_test).long()\n",
    "test_ds = td.TensorDataset(test_x,test_y)\n",
    "test_loader = td.DataLoader(test_ds, batch_size=20,\n",
    "                             shuffle=False, num_workers=1)\n",
    "print('Ready to load data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e56565d1-6391-48fe-b611-f9a98f63c2d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PenguinNet(\n  (fc1): Linear(in_features=5, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=3, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Number of hidden layer nodes\n",
    "hl = 10\n",
    "   \n",
    "# Define the neural network\n",
    "class PenguinNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PenguinNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(features), hl)\n",
    "        self.fc2 = nn.Linear(hl, hl)\n",
    "        self.fc3 = nn.Linear(hl, 3)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        fc1_output = torch.relu(self.fc1(x))\n",
    "        fc2_output = torch.relu(self.fc2(fc1_output))\n",
    "        y = F.log_softmax(self.fc3(fc2_output).float(), dim=1)\n",
    "        return y\n",
    "   \n",
    "# Create a model instance from the network\n",
    "model = PenguinNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f258a2-e79d-44e0-b9cd-a1a0d9bcdf4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "       \n",
    "    for batch, tensor in enumerate(data_loader):\n",
    "        data, target = tensor\n",
    "        #feedforward\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_criteria(out, target)\n",
    "        train_loss += loss.item()\n",
    "   \n",
    "        # backpropagate adjustments to the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "    #Return average loss\n",
    "    avg_loss = train_loss / (batch+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "              \n",
    "               \n",
    "def test(model, data_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for batch, tensor in enumerate(data_loader):\n",
    "            batch_count += 1\n",
    "            data, target = tensor\n",
    "            # Get the predictions\n",
    "            out = model(data)\n",
    "   \n",
    "            # calculate the loss\n",
    "            test_loss += loss_criteria(out, target).item()\n",
    "   \n",
    "            # Calculate the accuracy\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "               \n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss/batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n",
    "       \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49908bcf-92fb-47d9-824b-5d540e165f54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import horovod.torch as hvd\n",
    "from sparkdl import HorovodRunner\n",
    "   \n",
    "def train_hvd(model):\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "       \n",
    "    hvd.init()\n",
    "       \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if device.type == 'cuda':\n",
    "        # Pin GPU to local rank\n",
    "        torch.cuda.set_device(hvd.local_rank())\n",
    "       \n",
    "    # Configure the sampler so that each worker gets a distinct sample of the input dataset\n",
    "    train_sampler = DistributedSampler(train_ds, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "    # Use train_sampler to load a different sample of data on each worker\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=20, sampler=train_sampler)\n",
    "       \n",
    "    # The effective batch size in synchronous distributed training is scaled by the number of workers\n",
    "    # Increase learning_rate to compensate for the increased batch size\n",
    "    learning_rate = 0.001 * hvd.size()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "       \n",
    "    # Wrap the local optimizer with hvd.DistributedOptimizer so that Horovod handles the distributed optimization\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "   \n",
    "    # Broadcast initial parameters so all workers start with the same parameters\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "   \n",
    "    # Train over 50 epochs\n",
    "    epochs = 100\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        # Feed training data into the model to optimize the weights\n",
    "        train_loss = train(model, train_loader, optimizer)\n",
    "   \n",
    "    # Save the model weights\n",
    "    if hvd.rank() == 0:\n",
    "        model_file = '/dbfs/penguin_classifier_hvd.pt'\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "        print('model saved as', model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abacde04-b918-461a-b3a3-85202fc20d53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The global names read or written to by the pickled function are {'hvd': None, 'torch': None, 'train_ds': None, 'range': None, 'print': None, 'train': None}.\nThe pickled object size is 35743 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\n/databricks/spark/python/pyspark/sql/context.py:117: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\nStart training.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,0]<stderr>:/databricks/python/lib/python3.10/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n[1,0]<stderr>:  \"class\": algorithms.Blowfish,\n[1,1]<stderr>:/databricks/python/lib/python3.10/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n[1,1]<stderr>:  \"class\": algorithms.Blowfish,\n[1,0]<stderr>:/sbin/ldconfig.real: Can't link /lib/libhadoop.so.1.0.0 to libhadoop.so\n[1,0]<stderr>:/sbin/ldconfig.real: Renaming of /etc/ld.so.cache~ to /etc/ld.so.cache failed: No such file or directory\n[1,0]<stdout>:Epoch: 1\n[1,1]<stdout>:Epoch: 1\n[1,0]<stdout>:Training set: Average loss: 0.890916\n[1,0]<stdout>:Epoch: 2\n[1,1]<stdout>:Training set: Average loss: 0.910780\n[1,1]<stdout>:Epoch: 2\n[1,0]<stdout>:Training set: Average loss: 0.689195\n[1,0]<stdout>:Epoch: 3\n[1,1]<stdout>:Training set: Average loss: 0.680058\n[1,1]<stdout>:Epoch: 3\n[1,0]<stdout>:Training set: Average loss: 0.560151\n[1,0]<stdout>:Epoch: 4\n[1,1]<stdout>:Training set: Average loss: 0.554905\n[1,1]<stdout>:Epoch: 4\n[1,0]<stdout>:Training set: Average loss: 0.439088\n[1,0]<stdout>:Epoch: 5\n[1,1]<stdout>:Training set: Average loss: 0.433271\n[1,1]<stdout>:Epoch: 5\n[1,0]<stdout>:Training set: Average loss: 0.316075\n[1,0]<stdout>:Epoch: 6\n[1,1]<stdout>:Training set: Average loss: 0.308483\n[1,1]<stdout>:Epoch: 6\n[1,0]<stdout>:Training set: Average loss: 0.212125\n[1,0]<stdout>:Epoch: 7\n[1,1]<stdout>:Training set: Average loss: 0.203561\n[1,1]<stdout>:Epoch: 7\n[1,0]<stdout>:Training set: Average loss: 0.143811\n[1,0]<stdout>:Epoch: 8\n[1,1]<stdout>:Training set: Average loss: 0.135242\n[1,1]<stdout>:Epoch: 8\n[1,0]<stdout>:Training set: Average loss: 0.104120\n[1,0]<stdout>:Epoch: 9\n[1,1]<stdout>:Training set: Average loss: 0.096095\n[1,1]<stdout>:Epoch: 9\n[1,0]<stdout>:Training set: Average loss: 0.080678\n[1,0]<stdout>:Epoch: 10\n[1,1]<stdout>:Training set: Average loss: 0.074086\n[1,1]<stdout>:Epoch: 10\n[1,0]<stdout>:Training set: Average loss: 0.066031\n[1,0]<stdout>:Epoch: 11\n[1,1]<stdout>:Training set: Average loss: 0.060327\n[1,1]<stdout>:Epoch: 11\n[1,0]<stdout>:Training set: Average loss: 0.056197\n[1,1]<stdout>:Training set: Average loss: 0.051207\n[1,1]<stdout>:Epoch: 12\n[1,0]<stdout>:Epoch: 12\n[1,1]<stdout>:Training set: Average loss: 0.044765\n[1,1]<stdout>:Epoch: 13\n[1,0]<stdout>:Training set: Average loss: 0.049201\n[1,0]<stdout>:Epoch: 13\n[1,1]<stdout>:Training set: Average loss: 0.040025\n[1,1]<stdout>:Epoch: 14\n[1,0]<stdout>:Training set: Average loss: 0.043988\n[1,0]<stdout>:Epoch: 14\n[1,1]<stdout>:Training set: Average loss: 0.036396\n[1,1]<stdout>:Epoch: 15\n[1,0]<stdout>:Training set: Average loss: 0.039941\n[1,0]<stdout>:Epoch: 15\n[1,1]<stdout>:Training set: Average loss: 0.033521\n[1,1]<stdout>:Epoch: 16\n[1,0]<stdout>:Training set: Average loss: 0.036695\n[1,0]<stdout>:Epoch: 16\n[1,1]<stdout>:Training set: Average loss: 0.031186\n[1,1]<stdout>:Epoch: 17\n[1,0]<stdout>:Training set: Average loss: 0.034022\n[1,0]<stdout>:Epoch: 17\n[1,1]<stdout>:Training set: Average loss: 0.029274\n[1,1]<stdout>:Epoch: 18\n[1,0]<stdout>:Training set: Average loss: 0.031789\n[1,0]<stdout>:Epoch: 18\n[1,1]<stdout>:Training set: Average loss: 0.027626\n[1,1]<stdout>:Epoch: 19\n[1,0]<stdout>:Training set: Average loss: 0.029885\n[1,0]<stdout>:Epoch: 19\n[1,1]<stdout>:Training set: Average loss: 0.026206\n[1,1]<stdout>:Epoch: 20\n[1,0]<stdout>:Training set: Average loss: 0.028245\n[1,0]<stdout>:Epoch: 20\n[1,1]<stdout>:Training set: Average loss: 0.024976\n[1,1]<stdout>:Epoch: 21\n[1,0]<stdout>:Training set: Average loss: 0.026828\n[1,0]<stdout>:Epoch: 21\n[1,1]<stdout>:Training set: Average loss: 0.023894\n[1,1]<stdout>:Epoch: 22\n[1,0]<stdout>:Training set: Average loss: 0.025593\n[1,0]<stdout>:Epoch: 22\n[1,1]<stdout>:Training set: Average loss: 0.022940\n[1,1]<stdout>:Epoch: 23\n[1,0]<stdout>:Training set: Average loss: 0.024510\n[1,0]<stdout>:Epoch: 23\n[1,1]<stdout>:Training set: Average loss: 0.022089\n[1,1]<stdout>:Epoch: 24\n[1,0]<stdout>:Training set: Average loss: 0.023553\n[1,0]<stdout>:Epoch: 24\n[1,1]<stdout>:Training set: Average loss: 0.021324\n[1,1]<stdout>:Epoch: 25\n[1,0]<stdout>:Training set: Average loss: 0.022701\n[1,0]<stdout>:Epoch: 25\n[1,1]<stdout>:Training set: Average loss: 0.020633\n[1,0]<stdout>:Training set: Average loss: 0.021935\n[1,1]<stdout>:Epoch: 26\n[1,0]<stdout>:Epoch: 26\n[1,1]<stdout>:Training set: Average loss: 0.019997\n[1,0]<stdout>:Training set: Average loss: 0.021243\n[1,1]<stdout>:Epoch: 27\n[1,0]<stdout>:Epoch: 27\n[1,1]<stdout>:Training set: Average loss: 0.019415\n[1,1]<stdout>:Epoch: 28\n[1,0]<stdout>:Training set: Average loss: 0.020614\n[1,0]<stdout>:Epoch: 28\n[1,1]<stdout>:Training set: Average loss: 0.018885\n[1,1]<stdout>:Epoch: 29\n[1,0]<stdout>:Training set: Average loss: 0.020043\n[1,0]<stdout>:Epoch: 29\n[1,1]<stdout>:Training set: Average loss: 0.018404\n[1,1]<stdout>:Epoch: 30\n[1,0]<stdout>:Training set: Average loss: 0.019525\n[1,0]<stdout>:Epoch: 30\n[1,1]<stdout>:Training set: Average loss: 0.017925\n[1,1]<stdout>:Epoch: 31\n[1,0]<stdout>:Training set: Average loss: 0.019036\n[1,0]<stdout>:Epoch: 31\n[1,1]<stdout>:Training set: Average loss: 0.017490\n[1,1]<stdout>:Epoch: 32\n[1,0]<stdout>:Training set: Average loss: 0.018588\n[1,0]<stdout>:Epoch: 32\n[1,1]<stdout>:Training set: Average loss: 0.017083\n[1,1]<stdout>:Epoch: 33\n[1,0]<stdout>:Training set: Average loss: 0.018174\n[1,0]<stdout>:Epoch: 33\n[1,1]<stdout>:Training set: Average loss: 0.016695\n[1,1]<stdout>:Epoch: 34\n[1,0]<stdout>:Training set: Average loss: 0.017786\n[1,0]<stdout>:Epoch: 34\n[1,1]<stdout>:Training set: Average loss: 0.016330\n[1,1]<stdout>:Epoch: 35\n[1,0]<stdout>:Training set: Average loss: 0.017423\n[1,0]<stdout>:Epoch: 35\n[1,1]<stdout>:Training set: Average loss: 0.016013\n[1,1]<stdout>:Epoch: 36\n[1,0]<stdout>:Training set: Average loss: 0.017083\n[1,0]<stdout>:Epoch: 36\n[1,1]<stdout>:Training set: Average loss: 0.015684\n[1,1]<stdout>:Epoch: 37\n[1,0]<stdout>:Training set: Average loss: 0.016767\n[1,0]<stdout>:Epoch: 37\n[1,1]<stdout>:Training set: Average loss: 0.015372\n[1,1]<stdout>:Epoch: 38\n[1,0]<stdout>:Training set: Average loss: 0.016466\n[1,0]<stdout>:Epoch: 38\n[1,1]<stdout>:Training set: Average loss: 0.015077\n[1,1]<stdout>:Epoch: 39\n[1,0]<stdout>:Training set: Average loss: 0.016182\n[1,0]<stdout>:Epoch: 39\n[1,1]<stdout>:Training set: Average loss: 0.014789\n[1,1]<stdout>:Epoch: 40\n[1,0]<stdout>:Training set: Average loss: 0.015913\n[1,0]<stdout>:Epoch: 40\n[1,1]<stdout>:Training set: Average loss: 0.014573\n[1,1]<stdout>:Epoch: 41\n[1,0]<stdout>:Training set: Average loss: 0.015676\n[1,0]<stdout>:Epoch: 41\n[1,1]<stdout>:Training set: Average loss: 0.014268\n[1,1]<stdout>:Epoch: 42\n[1,0]<stdout>:Training set: Average loss: 0.015417\n[1,0]<stdout>:Epoch: 42\n[1,1]<stdout>:Training set: Average loss: 0.014025\n[1,1]<stdout>:Epoch: 43\n[1,0]<stdout>:Training set: Average loss: 0.015185\n[1,0]<stdout>:Epoch: 43\n[1,1]<stdout>:Training set: Average loss: 0.013737\n[1,1]<stdout>:Epoch: 44\n[1,0]<stdout>:Training set: Average loss: 0.014952\n[1,0]<stdout>:Epoch: 44\n[1,1]<stdout>:Training set: Average loss: 0.013588\n[1,1]<stdout>:Epoch: 45\n[1,0]<stdout>:Training set: Average loss: 0.014770\n[1,0]<stdout>:Epoch: 45\n[1,1]<stdout>:Training set: Average loss: 0.013316\n[1,0]<stdout>:Training set: Average loss: 0.014553\n[1,0]<stdout>:Epoch: 46\n[1,1]<stdout>:Epoch: 46\n[1,0]<stdout>:Training set: Average loss: 0.014359\n[1,0]<stdout>:Epoch: 47\n[1,1]<stdout>:Training set: Average loss: 0.013108\n[1,1]<stdout>:Epoch: 47\n[1,0]<stdout>:Training set: Average loss: 0.014175\n[1,0]<stdout>:Epoch: 48\n[1,1]<stdout>:Training set: Average loss: 0.012897\n[1,1]<stdout>:Epoch: 48\n[1,0]<stdout>:Training set: Average loss: 0.013996\n[1,0]<stdout>:Epoch: 49\n[1,1]<stdout>:Training set: Average loss: 0.012668\n[1,1]<stdout>:Epoch: 49\n[1,0]<stdout>:Training set: Average loss: 0.013824\n[1,0]<stdout>:Epoch: 50\n[1,1]<stdout>:Training set: Average loss: 0.012477\n[1,1]<stdout>:Epoch: 50\n[1,0]<stdout>:Training set: Average loss: 0.013663\n[1,0]<stdout>:Epoch: 51\n[1,1]<stdout>:Training set: Average loss: 0.012311\n[1,1]<stdout>:Epoch: 51\n[1,0]<stdout>:Training set: Average loss: 0.013505\n[1,0]<stdout>:Epoch: 52\n[1,1]<stdout>:Training set: Average loss: 0.012119\n[1,1]<stdout>:Epoch: 52\n[1,0]<stdout>:Training set: Average loss: 0.013350\n[1,0]<stdout>:Epoch: 53\n[1,1]<stdout>:Training set: Average loss: 0.011937\n[1,1]<stdout>:Epoch: 53\n[1,0]<stdout>:Training set: Average loss: 0.013202\n[1,0]<stdout>:Epoch: 54\n[1,1]<stdout>:Training set: Average loss: 0.011761\n[1,1]<stdout>:Epoch: 54\n[1,0]<stdout>:Training set: Average loss: 0.013057\n[1,0]<stdout>:Epoch: 55\n[1,1]<stdout>:Training set: Average loss: 0.011567\n[1,1]<stdout>:Epoch: 55\n[1,0]<stdout>:Training set: Average loss: 0.012920\n[1,0]<stdout>:Epoch: 56\n[1,1]<stdout>:Training set: Average loss: 0.011429\n[1,1]<stdout>:Epoch: 56\n[1,0]<stdout>:Training set: Average loss: 0.012787\n[1,0]<stdout>:Epoch: 57\n[1,1]<stdout>:Training set: Average loss: 0.011261\n[1,1]<stdout>:Epoch: 57\n[1,0]<stdout>:Training set: Average loss: 0.012655\n[1,0]<stdout>:Epoch: 58\n[1,1]<stdout>:Training set: Average loss: 0.011101\n[1,1]<stdout>:Epoch: 58\n[1,0]<stdout>:Training set: Average loss: 0.012529\n[1,0]<stdout>:Epoch: 59\n[1,1]<stdout>:Training set: Average loss: 0.010946\n[1,1]<stdout>:Epoch: 59\n[1,1]<stdout>:Training set: Average loss: 0.010794[1,0]<stdout>:Training set: Average loss: 0.012406\n[1,0]<stdout>:Epoch: 60\n[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 60[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.012286\n[1,0]<stdout>:Epoch: 61\n[1,1]<stdout>:Training set: Average loss: 0.010646[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 61[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.012170\n[1,0]<stdout>:Epoch: 62\n[1,1]<stdout>:Training set: Average loss: 0.010501[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 62[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.012057\n[1,0]<stdout>:Epoch: 63\n[1,1]<stdout>:Training set: Average loss: 0.010360[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 63[1,1]<stdout>:\n[1,1]<stdout>:Training set: Average loss: 0.010221[1,0]<stdout>:Training set: Average loss: 0.011947\n[1,0]<stdout>:Epoch: 64\n[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 64[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.011840\n[1,0]<stdout>:Epoch: 65\n[1,1]<stdout>:Training set: Average loss: 0.010085[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 65[1,1]<stdout>:\n[1,1]<stdout>:Training set: Average loss: 0.009952[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.011736\n[1,0]<stdout>:Epoch: 66\n[1,1]<stdout>:Epoch: 66[1,1]<stdout>:\n[1,1]<stdout>:Training set: Average loss: 0.009822[1,0]<stdout>:Training set: Average loss: 0.011634\n[1,0]<stdout>:Epoch: 67\n[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 67[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.011535\n[1,0]<stdout>:Epoch: 68\n[1,1]<stdout>:Training set: Average loss: 0.009694[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 68[1,1]<stdout>:\n[1,1]<stdout>:Training set: Average loss: 0.009569[1,0]<stdout>:Training set: Average loss: 0.011438\n[1,0]<stdout>:Epoch: 69\n[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 69[1,1]<stdout>:\n[1,1]<stdout>:Training set: Average loss: 0.009446[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.011343\n[1,0]<stdout>:Epoch: 70\n[1,1]<stdout>:Epoch: 70[1,1]<stdout>:\n[1,1]<stdout>:Training set: Average loss: 0.009325[1,0]<stdout>:Training set: Average loss: 0.011250\n[1,0]<stdout>:Epoch: 71\n[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 71[1,1]<stdout>:\n[1,1]<stdout>:Training set: Average loss: 0.009207[1,0]<stdout>:Training set: Average loss: 0.011160\n[1,0]<stdout>:Epoch: 72\n[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 72[1,1]<stdout>:\n[1,1]<stdout>:Training set: Average loss: 0.009090[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 73[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.011072\n[1,0]<stdout>:Epoch: 73\n[1,1]<stdout>:Training set: Average loss: 0.008976[1,0]<stdout>:Training set: Average loss: 0.010985\n[1,0]<stdout>:Epoch: 74\n[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 74[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.010901\n[1,0]<stdout>:Epoch: 75\n[1,1]<stdout>:Training set: Average loss: 0.008864[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 75[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.010818\n[1,0]<stdout>:Epoch: 76\n[1,1]<stdout>:Training set: Average loss: 0.008753[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 76[1,1]<stdout>:\n[1,0]<stdout>:Training set: Average loss: 0.010737\n[1,0]<stdout>:Epoch: 77\n[1,1]<stdout>:Training set: Average loss: 0.008645[1,1]<stdout>:\n[1,1]<stdout>:Epoch: 77[1,1]<stdout>:\n[1,1]<stdout>:Training set: Average loss: 0.008538\n[1,1]<stdout>:Epoch: 78\n[1,0]<stdout>:Training set: Average loss: 0.010658\n[1,0]<stdout>:Epoch: 78\n[1,1]<stdout>:Training set: Average loss: 0.008433\n[1,1]<stdout>:Epoch: 79\n[1,0]<stdout>:Training set: Average loss: 0.010580\n[1,0]<stdout>:Epoch: 79\n[1,0]<stdout>:Training set: Average loss: 0.010504\n[1,0]<stdout>:Epoch: 80\n[1,1]<stdout>:Training set: Average loss: 0.008330\n[1,1]<stdout>:Epoch: 80\n[1,0]<stdout>:Training set: Average loss: 0.010429\n[1,0]<stdout>:Epoch: 81\n[1,1]<stdout>:Training set: Average loss: 0.008228\n[1,1]<stdout>:Epoch: 81\n[1,0]<stdout>:Training set: Average loss: 0.010356\n[1,0]<stdout>:Epoch: 82\n[1,1]<stdout>:Training set: Average loss: 0.008129\n[1,1]<stdout>:Epoch: 82\n[1,0]<stdout>:Training set: Average loss: 0.010284\n[1,0]<stdout>:Epoch: 83\n[1,1]<stdout>:Training set: Average loss: 0.008043\n[1,1]<stdout>:Epoch: 83\n[1,0]<stdout>:Training set: Average loss: 0.010212\n[1,0]<stdout>:Epoch: 84\n[1,1]<stdout>:Training set: Average loss: 0.007931\n[1,1]<stdout>:Epoch: 84\n[1,0]<stdout>:Training set: Average loss: 0.010142\n[1,0]<stdout>:Epoch: 85\n[1,1]<stdout>:Training set: Average loss: 0.007844\n[1,1]<stdout>:Epoch: 85\n[1,0]<stdout>:Training set: Average loss: 0.010076\n[1,0]<stdout>:Epoch: 86\n[1,1]<stdout>:Training set: Average loss: 0.007747\n[1,1]<stdout>:Epoch: 86\n[1,0]<stdout>:Training set: Average loss: 0.010009\n[1,0]<stdout>:Epoch: 87\n[1,1]<stdout>:Training set: Average loss: 0.007649\n[1,1]<stdout>:Epoch: 87\n[1,0]<stdout>:Training set: Average loss: 0.009945\n[1,0]<stdout>:Epoch: 88\n[1,1]<stdout>:Training set: Average loss: 0.007567\n[1,1]<stdout>:Epoch: 88\n[1,0]<stdout>:Training set: Average loss: 0.009881\n[1,0]<stdout>:Epoch: 89\n[1,1]<stdout>:Training set: Average loss: 0.007468\n[1,1]<stdout>:Epoch: 89\n[1,0]<stdout>:Training set: Average loss: 0.009819\n[1,0]<stdout>:Epoch: 90\n[1,1]<stdout>:Training set: Average loss: 0.007389\n[1,1]<stdout>:Epoch: 90\n[1,0]<stdout>:Training set: Average loss: 0.009756\n[1,0]<stdout>:Epoch: 91\n[1,1]<stdout>:Training set: Average loss: 0.007302\n[1,1]<stdout>:Epoch: 91\n[1,0]<stdout>:Training set: Average loss: 0.009695\n[1,0]<stdout>:Epoch: 92\n[1,1]<stdout>:Training set: Average loss: 0.007209\n[1,1]<stdout>:Epoch: 92\n[1,0]<stdout>:Training set: Average loss: 0.009634\n[1,0]<stdout>:Epoch: 93\n[1,1]<stdout>:Training set: Average loss: 0.007128\n[1,1]<stdout>:Epoch: 93\n[1,0]<stdout>:Training set: Average loss: 0.009577\n[1,0]<stdout>:Epoch: 94\n[1,1]<stdout>:Training set: Average loss: 0.007043\n[1,1]<stdout>:Epoch: 94\n[1,0]<stdout>:Training set: Average loss: 0.009518\n[1,0]<stdout>:Epoch: 95\n[1,1]<stdout>:Training set: Average loss: 0.006960\n[1,1]<stdout>:Epoch: 95\n[1,0]<stdout>:Training set: Average loss: 0.009461\n[1,0]<stdout>:Epoch: 96\n[1,1]<stdout>:Training set: Average loss: 0.006874\n[1,1]<stdout>:Epoch: 96\n[1,0]<stdout>:Training set: Average loss: 0.009406\n[1,0]<stdout>:Epoch: 97\n[1,1]<stdout>:Training set: Average loss: 0.006801\n[1,1]<stdout>:Epoch: 97\n[1,0]<stdout>:Training set: Average loss: 0.009350\n[1,0]<stdout>:Epoch: 98\n[1,1]<stdout>:Training set: Average loss: 0.006721\n[1,1]<stdout>:Epoch: 98\n[1,0]<stdout>:Training set: Average loss: 0.009296\n[1,0]<stdout>:Epoch: 99\n[1,1]<stdout>:Training set: Average loss: 0.006639\n[1,1]<stdout>:Epoch: 99\n[1,0]<stdout>:Training set: Average loss: 0.009241\n[1,0]<stdout>:Epoch: 100\n[1,1]<stdout>:Training set: Average loss: 0.006564\n[1,1]<stdout>:Epoch: 100\n[1,0]<stdout>:Training set: Average loss: 0.009190\n[1,1]<stdout>:Training set: Average loss: 0.006487\n[1,0]<stdout>:model saved as /dbfs/penguin_classifier_hvd.pt\nValidation set: Average loss: 0.005886, Accuracy: 410/411 (100%)\n\n"
     ]
    }
   ],
   "source": [
    "# Reset random seed for PyTorch\n",
    "torch.manual_seed(0)\n",
    "   \n",
    "# Create a new model\n",
    "new_model = PenguinNet()\n",
    "   \n",
    "# We'll use CrossEntropyLoss to optimize a multiclass classifier\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "   \n",
    "# Run the distributed training function on 2 nodes\n",
    "hr = HorovodRunner(np=2, driver_log_verbosity='all') \n",
    "hr.run(train_hvd, model=new_model)\n",
    "   \n",
    "# Load the trained weights and test the model\n",
    "test_model = PenguinNet()\n",
    "test_model.load_state_dict(torch.load('/dbfs/penguin_classifier_hvd.pt'))\n",
    "test_loss = test(test_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cc19483-95f4-4943-ac9a-ea719ff1e4d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2206067441996279,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Deep Learning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
